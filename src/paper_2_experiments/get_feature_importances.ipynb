{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc5c836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import sys, os\n",
    "from os.path import join\n",
    "sys.path.append('/work/mflora/ROAD_SURFACE')\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/scikit-explain')\n",
    "\n",
    "\n",
    "from skexplain.plot.base_plotting import PlotStructure\n",
    "from matplotlib.lines import Line2D\n",
    "from skexplain import ExplainToolkit\n",
    "from skexplain.common.importance_utils import to_skexplain_importance\n",
    "\n",
    "from ml_workflow.ml_methods import norm_aupdc, brier_skill_score\n",
    "from ml_workflow import CalibratedPipelineHyperOptCV\n",
    "from ml_workflow.ml_methods import get_bootstrap_score\n",
    "from calibration_classifier import CalibratedClassifier\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from wofs_ml_severe.wofs_ml_severe.common.multiprocessing_utils import run_parallel, to_iterator\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import pickle\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import hyperopt.hp as hp\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "sns.set_theme(palette=sns.color_palette(\"Set2\"))\n",
    "\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from probsr_config import TARGET_COLUMN, PREDICTOR_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74b8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = f'../models/RandomForest_manualfeatures_12.joblib'\n",
    "#model = load(model_name)['model'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5084e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc5f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/work/mflora/explainability_work/'\n",
    "DATA_BASE_PATH = join(BASE_PATH, 'datasets')\n",
    "MODEL_BASE_PATH = join(BASE_PATH, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f1a3b",
   "metadata": {},
   "source": [
    "## Create the Association between feature importance and model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f38c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the respective ranking method results\n",
    "other_base_path = '/work/mflora/explainability_work/results'\n",
    "def get_feature_importances(target, opt, method, results=None):\n",
    "    \n",
    "    time='first_hour'\n",
    "    method = 'unnormalized'\n",
    "    explainer = ExplainToolkit()\n",
    "    if target == 'road_surface':\n",
    "        ###opt = 'reduced'\n",
    "        est_name = 'Random Forest'\n",
    "        if opt == 'reduced':\n",
    "            model_name = join(MODEL_BASE_PATH,'RandomForest_manualfeatures_12.joblib')\n",
    "            model = load(model_name)['model'] \n",
    "            data = load(model_name)\n",
    "            model = data['model'] \n",
    "            features = data['features']\n",
    "            gini_values = model.base_estimator.named_steps['model'].feature_importances_\n",
    "        else:\n",
    "            model_name = join(MODEL_BASE_PATH,'JTTI_ProbSR_RandomForest.pkl')\n",
    "            model = load(model_name)\n",
    "            gini_values = model.feature_importances_\n",
    "            features = PREDICTOR_COLUMNS\n",
    "        \n",
    "        pimp_backward_fname = join(DATA_BASE_PATH, f'perm_imp_{opt}_backward.nc')\n",
    "        pimp_forward_fname = join(DATA_BASE_PATH, f'perm_imp_{opt}_forward.nc')\n",
    "        ale_var_fname = join(DATA_BASE_PATH, f'ale_var_rf_{opt}.nc')\n",
    "        shap_fname = join(DATA_BASE_PATH, f'shap_rf_{opt}.nc')\n",
    "\n",
    "        gini_rank = to_skexplain_importance(gini_values,\n",
    "                                       estimator_name='Random Forest', \n",
    "                                       feature_names=features, \n",
    "                                         method = 'gini')\n",
    "        \n",
    "    else:\n",
    "        est_name = 'LogisticRegression'\n",
    "        model_name = join(MODEL_BASE_PATH, \n",
    "                          f'LogisticRegression_first_hour_{target}_under_standard_{opt}.pkl')\n",
    "        data = load(model_name)\n",
    "        model = data['model']\n",
    "        features = data['features']\n",
    "    \n",
    "        pimp_backward_fname = join(DATA_BASE_PATH,\n",
    "                        f'permutation_importance_{target}_{time}_training_norm_aupdc{opt}backward.nc')\n",
    "        pimp_forward_fname = join(DATA_BASE_PATH, \n",
    "                        f'permutation_importance_{target}_{time}_training_norm_aupdc{opt}forward.nc')\n",
    "        ale_var_fname = join(DATA_BASE_PATH,\n",
    "                        f'ale_var_results_all_models_{target}_{time}{opt}.nc')\n",
    "        shap_fname = join(DATA_BASE_PATH,\n",
    "                        f'shap_values_LogisticRegression_{target}_{time}{opt}.pkl')\n",
    "\n",
    "        coefs = model.base_estimator.named_steps['model'].coef_[0]\n",
    "        coef_rank = to_skexplain_importance(coefs,\n",
    "                                       estimator_name='LogisticRegression', \n",
    "                                       feature_names=features, \n",
    "                                       method = 'coefs')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    sage_opt = 'reduced' if ('L1_based' in opt or opt=='reduced') else 'original'\n",
    "    sage_fname = join(DATA_BASE_PATH,f'sage_results_{sage_opt}_{target}.nc')\n",
    "    ale_var = explainer.load(ale_var_fname)\n",
    "    pimp_backward = explainer.load(pimp_backward_fname)\n",
    "    pimp_forward = explainer.load(pimp_forward_fname)\n",
    "    \n",
    "    with open(shap_fname, 'rb') as pkl_file:\n",
    "        data = pickle.load(pkl_file)\n",
    "\n",
    "    X = data['X']\n",
    "    shap_vals = data['shap_values']\n",
    "    shap_rank = to_skexplain_importance(shap_vals, \n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method ='shap_sum')\n",
    "\n",
    "    with open(sage_fname, 'rb') as f:\n",
    "        sage_results = pickle.load(f)\n",
    "    \n",
    "    sage_rank = to_skexplain_importance(sage_results,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage')\n",
    "    \n",
    "    # Lime\n",
    "    lime_fname = join(other_base_path,f'lime_rank_{target}_{sage_opt}.nc')\n",
    "    lime_rank = explainer.load(lime_fname)\n",
    "    \n",
    "    # TI \n",
    "    if target=='road_surface':\n",
    "        ti_fname = join(other_base_path,f'ti_rank_{target}_{sage_opt}.nc')\n",
    "        ti_rank = explainer.load(ti_fname)\n",
    "    \n",
    "    \n",
    "    data = [pimp_backward, \n",
    "            pimp_backward,\n",
    "            pimp_forward, \n",
    "            pimp_forward,\n",
    "            ale_var,  \n",
    "            shap_rank,\n",
    "            sage_rank,\n",
    "            lime_rank\n",
    "           ]\n",
    "    \n",
    "    methods = ['multipass__backward', \n",
    "           'singlepass__backward', \n",
    "           'multipass__forward', \n",
    "           'singlepass__forward', \n",
    "           'ale_variance', \n",
    "           'shap_sum',\n",
    "           'sage',\n",
    "           'lime'\n",
    "           ]\n",
    "\n",
    "    if target == 'road_surface':\n",
    "        data.append(ti_rank)\n",
    "        methods.append('tree_interpreter')\n",
    "    \n",
    "    if target == 'road_surface': \n",
    "        data.append(gini_rank)\n",
    "        methods.append('gini')\n",
    "    else:\n",
    "        data.append(coef_rank)\n",
    "        methods.append('coefs')\n",
    "    \n",
    "    # Turn the permutation importances into proper importance scores. \n",
    "    # Backward multipass and forward singlepass: 1 + (permute score - original score)\n",
    "    original_score = pimp_backward[f'original_score__{est_name}'].values\n",
    "    scores = 1.0 + (pimp_backward[f'multipass_scores__{est_name}'].values - original_score)\n",
    "    pimp_backward[f'multipass_scores__{est_name}'] = (['n_vars_multipass', 'n_permute'], scores)\n",
    "    \n",
    "    original_score = pimp_forward[f'original_score__{est_name}'].values\n",
    "    scores = 1.0 + (pimp_forward[f'singlepass_scores__{est_name}'].values - original_score)\n",
    "    pimp_forward[f'singlepass_scores__{est_name}'] = (['n_vars_singlepass', 'n_permute'], scores)\n",
    "  \n",
    "    print(pimp_backward[f'multipass_scores__{est_name}'].shape)\n",
    "    print(pimp_forward[f'multipass_scores__{est_name}'].shape)\n",
    "    \n",
    "    # Backward singlepass and forward multipass: original_score - permuted score\n",
    "    original_score = pimp_backward[f'original_score__{est_name}'].values\n",
    "    scores = original_score - pimp_backward[f'singlepass_scores__{est_name}'].values\n",
    "    pimp_backward[f'singlepass_scores__{est_name}'] = (['n_vars_singlepass', 'n_permute'], scores)\n",
    "    \n",
    "    original_score = pimp_forward[f'original_score__{est_name}'].values\n",
    "    scores = original_score - pimp_forward[f'multipass_scores__{est_name}'].values\n",
    "    pimp_forward[f'multipass_scores__{est_name}'] = (['n_vars_multipass', 'n_permute'], scores)\n",
    "    \n",
    "    #data = [shap_rank, pimp_backward]\n",
    "    #methods = ['shap_sum', 'multipass__backward']\n",
    "    \n",
    "    feature_importances = {m : pd.DataFrame(ds[f\"{m.split('__')[0]}_scores__{est_name}\"].values.T, \n",
    "                       columns = ds[f\"{m.split('__')[0]}_rankings__{est_name}\"]) \n",
    "                           for m,ds in zip(methods, data) }\n",
    "    \n",
    "    #for key in feature_importances.keys():\n",
    "    #    print(key)\n",
    "    #    print(f'{feature_importances[key]=}')\n",
    "    \n",
    "    if results is not None:\n",
    "        feature_subsets = results['subsets']\n",
    "        n_iter = len(feature_subsets)\n",
    "        importances = {m : np.zeros((n_iter)) for m in methods}\n",
    "         # Iterate over several thousand feature subsets\n",
    "        for i, inds in enumerate(feature_subsets):\n",
    "            for m in methods:\n",
    "                # Sum up the importances from the different bootstrapping and then average. \n",
    "                # and then normalize by the feature set size. \n",
    "                if method == 'normalized':\n",
    "                    factor =  float((len(features) - len(inds)) / len(features))\n",
    "                elif method == 'normalized_divide':\n",
    "                    factor = 1.0 / len(inds)\n",
    "                else:\n",
    "                    factor = 1.0\n",
    "                \n",
    "                val = np.sum(np.mean(feature_importances[m][inds], axis=0)) * factor\n",
    "                \n",
    "                importances[m][i] = val\n",
    "    else:\n",
    "        importances = None\n",
    "    \n",
    "    return feature_importances, importances "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b995c",
   "metadata": {},
   "source": [
    "### Train models in the top and bottom 15 predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c088fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_and_bottom_predictors(target, opt):\n",
    "    N_FEATURES = 15\n",
    "    \n",
    "    model_opt = 'L1_based_feature_selection_with_manual' if opt == 'reduced' else ''\n",
    "    \n",
    "    data = {}\n",
    "    time = 'first_hour'\n",
    "    param_grid = {\n",
    "                'l1_ratio': hp.choice('l1_ratio', [0.0001, 0.001, 0.01, 0.1, 0.5, 0.6, 0.8, 1.0]),\n",
    "                'C': hp.choice('C', [0.0001, 0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0]),\n",
    "                }\n",
    "\n",
    "    save_fname = join(DATA_BASE_PATH, f'top_worst_predictors_{target}_{model_opt}_scores.pkl')\n",
    "\n",
    "\n",
    "    print(f'Working on {target}...')\n",
    "    # Load training dataset\n",
    "    #df = pd.read_pickle(f'../datasets/{time}_training_matched_to_{target}_0km_dataset')\n",
    "    #dates = df['Run Date']\n",
    "    #fti = df['FCST_TIME_IDX'].astype(int) \n",
    "    \n",
    "    model_fname = join(MODEL_BASE_PATH,\n",
    "                           f'LogisticRegression_first_hour_{target}_under_standard_{model_opt}.pkl')\n",
    "    \n",
    "    data = load(model_fname)\n",
    "    original_features = data['features']\n",
    "    model = data['model']\n",
    "    params = model.base_estimator.named_steps['model'].get_params()\n",
    "    params['max_iter'] = 300\n",
    "    \n",
    "    #X = df[original_features].astype(float)\n",
    "    #y = df[f'matched_to_{target}_0km'].astype(float).values \n",
    "    #known_skew = np.mean(y)\n",
    "    known_skew = 0.05\n",
    "    dates = []\n",
    "        \n",
    "    fname = join(DATA_BASE_PATH, f'importances_{target}_{model_opt}.pkl')\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = pickle.load(f)     \n",
    "    \n",
    "    feature_importances = data['feature_importances']\n",
    "        \n",
    "    feature_importances, _ = get_feature_importances(target, opt, results=None,)\n",
    "    results = {m:{} for m in feature_importances.keys() }\n",
    "    \n",
    "    for m in feature_importances.keys():\n",
    "        print('*'*40)\n",
    "        print(f'\\nMethod : {m}\\n')\n",
    "        print('*'*40)\n",
    "        imp = feature_importances[m]\n",
    "        \n",
    "        best_5 = list(feature_importances[m].columns[:N_FEATURES])\n",
    "        worst_5 = list(feature_importances[m].columns[-N_FEATURES:])\n",
    "        \n",
    "        estimator = LogisticRegression(solver='saga', \n",
    "                                       penalty='elasticnet', max_iter=300, random_state=42)\n",
    "\n",
    "        top_scores = []\n",
    "        for i, _ in enumerate(best_5):\n",
    "            clf = CalibratedPipelineHyperOptCV( base_estimator = estimator,\n",
    "                                  param_grid = param_grid,\n",
    "                                  resample='under',\n",
    "                                  scaler='standard',\n",
    "                                  imputer=None,\n",
    "                                  hyperopt='atpe',\n",
    "                                  n_jobs=1,\n",
    "                                  max_iter=20,\n",
    "                                  scorer_kwargs = {'known_skew': known_skew },\n",
    "                                  cv = 'date_based',\n",
    "                                  cv_kwargs = {'n_splits' : 5,\n",
    "                                               'dates' : dates,\n",
    "                                               'valid_size' : 20 },\n",
    "                                  )\n",
    "            \n",
    "            \n",
    "            feature_subset = best_5[:i+1]\n",
    "            print('\\n', feature_subset)\n",
    "            X_subset = X[feature_subset]\n",
    "            clf.fit(X_subset, y, params=params)\n",
    "            preds = clf.predict_proba(X_subset)[:,1]\n",
    "            ds = get_bootstrap_score(y, preds, n_bootstrap=100, \n",
    "                                     known_skew = known_skew, \n",
    "                                     forecast_time_indices = fti,\n",
    "                                     metric_mapper={'naupdc' : norm_aupdc})\n",
    "            top_scores.append(ds)\n",
    "            \n",
    "        bottom_scores = []\n",
    "        for i, _ in enumerate(worst_5[::-1]):\n",
    "            clf = CalibratedPipelineHyperOptCV( base_estimator = estimator,\n",
    "                                  param_grid = param_grid,\n",
    "                                  resample='under',\n",
    "                                  scaler='standard',\n",
    "                                  imputer=None,\n",
    "                                  hyperopt='atpe',\n",
    "                                  n_jobs=1,\n",
    "                                  max_iter=20,\n",
    "                                  scorer_kwargs = {'known_skew': known_skew },\n",
    "                                  cv = 'date_based',\n",
    "                                  cv_kwargs = {'n_splits' : 5,\n",
    "                                               'dates' : dates,\n",
    "                                               'valid_size' : 20 },\n",
    "                                  )\n",
    "            feature_subset = worst_5[::-1][:i+1]\n",
    "            X_subset = X[feature_subset]\n",
    "            clf.fit(X_subset, y, params=params)\n",
    "            preds = clf.predict_proba(X_subset)[:,1]\n",
    "            ds = get_bootstrap_score(y, preds, n_bootstrap=100, \n",
    "                                     known_skew = known_skew, \n",
    "                                     forecast_time_indices = fti ,\n",
    "                                     metric_mapper={'naupdc' : norm_aupdc})\n",
    "            \n",
    "            bottom_scores.append(ds)\n",
    "            \n",
    "        results[m]['top_scores'] = top_scores\n",
    "        results[m]['bottom_scores'] = bottom_scores\n",
    "        \n",
    "    with open(save_fname, 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64737f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on tornado...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/mflora/explainability_work/datasets/importances_tornado_L1_based_feature_selection_with_manual.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_top_and_bottom_predictors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtornado\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreduced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mtrain_top_and_bottom_predictors\u001b[0;34m(target, opt)\u001b[0m\n\u001b[1;32m     35\u001b[0m dates \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m fname \u001b[38;5;241m=\u001b[39m join(DATA_BASE_PATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportances_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_opt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     39\u001b[0m     data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)     \n\u001b[1;32m     41\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_importances\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/mflora/explainability_work/datasets/importances_tornado_L1_based_feature_selection_with_manual.pkl'"
     ]
    }
   ],
   "source": [
    "train_top_and_bottom_predictors('tornado', 'reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c74d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_top_and_bottom_predictors()\n",
    "#targets= ['tornado', 'severe_hail', 'severe_wind', 'road_surface']\n",
    "#opts = ['L1_based_feature_selection_with_manual', '']\n",
    "\n",
    "run_parallel(train_top_and_bottom_predictors, \n",
    "#            args_iterator=to_iterator(targets, opts), \n",
    "#             nprocs_to_use=len(targets)*len(opts), \n",
    "#             description='Feature Importance Experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82219e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_top_predictors(target, opt):\n",
    "    param_grid = {\n",
    "                'l1_ratio': hp.choice('l1_ratio', [0.0001, 0.001, 0.01, 0.1, 0.5, 0.6, 0.8, 1.0]),\n",
    "                'C': hp.choice('C', [0.0001, 0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0]),\n",
    "                }\n",
    "\n",
    "        \n",
    "    if target == 'road_surface':\n",
    "        fti = None\n",
    "        opt = 'original' if opt == '' else 'reduced'\n",
    "        print(f'Working on {target} {opt}...')\n",
    "        df = pd.read_csv('../datasets/probsr_training_data.csv')\n",
    "        random_state = np.random.RandomState(123)\n",
    "        inds = random_state.choice(len(df), size=300000, replace=False)\n",
    "        dates = pd.to_datetime(df['date'], errors ='coerce')\n",
    "\n",
    "        dates.astype('int64').dtypes\n",
    "        # extracting the week from the date\n",
    "        weekNumber = dates.dt.isocalendar().week\n",
    "        weekNumber = weekNumber.values[inds]\n",
    "        \n",
    "        \n",
    "        if opt == 'reduced':\n",
    "            model_name = f'../models/RandomForest_manualfeatures_12.joblib'\n",
    "            model = load(model_name)['model'] \n",
    "            data = load(model_name)\n",
    "            model = data['model'] \n",
    "            features = data['features']\n",
    "                \n",
    "            X = df[features].astype(float)\n",
    "            X = X.iloc[inds, :]\n",
    "            X.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "            y = df[TARGET_COLUMN].astype(float)\n",
    "            y = y.values[inds]\n",
    "            params = model.base_estimator.named_steps['model'].get_params()\n",
    "                \n",
    "            else:\n",
    "                model_fname = '../models/JTTI_ProbSR_RandomForest.pkl'\n",
    "                calibrator_fname = '../models/JTTI_ProbSR_RandomForest_Isotonic.pkl'\n",
    "        \n",
    "                base_estimator = load(model_fname)\n",
    "                calibrator = load(calibrator_fname)\n",
    "                model = CalibratedClassifier(base_estimator, calibrator)\n",
    "                params = base_estimator.get_params()\n",
    "        \n",
    "                X = df[PREDICTOR_COLUMNS]\n",
    "                X = X.iloc[inds, :]\n",
    "                X.reset_index(drop=True, inplace=True)\n",
    "                y = df[TARGET_COLUMN]\n",
    "                y = y.values[inds]\n",
    "\n",
    "    else:\n",
    "        print(f'Working on {target} {opt}...')\n",
    "\n",
    "        # Load training dataset\n",
    "        df = pd.read_pickle(f'../datasets/{time}_training_matched_to_{target}_0km_dataset')\n",
    "        dates = df['Run Date']\n",
    "        fti = df['FCST_TIME_IDX'].astype(int) \n",
    "    \n",
    "        model_fname = f'../models/LogisticRegression_first_hour_{target}_under_standard_{opt}.pkl'\n",
    "    \n",
    "        data = load(model_fname)\n",
    "        original_features = data['features']\n",
    "        model = data['model']\n",
    "        params = model.base_estimator.named_steps['model'].get_params()\n",
    "        params['max_iter'] = 300\n",
    "    \n",
    "        X = df[original_features].astype(float)\n",
    "        y = df[f'matched_to_{target}_0km'].astype(float).values\n",
    "    \n",
    "    known_skew = np.mean(y)\n",
    "\n",
    "    feature_importances, _ = get_feature_importances(target, opt, results=None,)\n",
    "    results = {m:{} for m in feature_importances.keys() }\n",
    "    \n",
    "    n_features=10\n",
    "    for key in ['top_scores', 'bottom_scores']:\n",
    "        for m in feature_importances.keys():\n",
    "            print(f'Method : {m}')\n",
    "            imp = feature_importances[m]\n",
    "            \n",
    "            if key == 'top_scores':\n",
    "                best_features = list(imp.columns[:n_features])\n",
    "            else:\n",
    "                best_features = list(imp.columns[-n_features:])\n",
    "                \n",
    "            if target == 'road_surface':\n",
    "                estimator = RandomForestClassifier(n_jobs=10, \n",
    "                                                   random_state=30, \n",
    "                                                   criterion = 'entropy', \n",
    "                                                   class_weight = 'balanced')\n",
    "                    cv_kwargs = {'n_splits' : 5,\n",
    "                            'dates' : weekNumber,\n",
    "                             'valid_size' : 8 }\n",
    "                else:\n",
    "                    estimator = LogisticRegression(solver='saga', \n",
    "                                       penalty='elasticnet', max_iter=300, random_state=42)\n",
    "                    cv_kwargs = {'n_splits' : 5,\n",
    "                             'dates' : dates,\n",
    "                             'valid_size' : 20 }\n",
    "\n",
    "                scores = []\n",
    "                clf = CalibratedPipelineHyperOptCV( base_estimator = estimator,\n",
    "                                  param_grid = param_grid,\n",
    "                                  resample='under',\n",
    "                                  scaler='standard',\n",
    "                                  imputer=None,\n",
    "                                  hyperopt='atpe',\n",
    "                                  n_jobs=1,\n",
    "                                  max_iter=20,\n",
    "                                  scorer_kwargs = {'known_skew': known_skew },\n",
    "                                  cv = 'date_based',\n",
    "                                  cv_kwargs = cv_kwargs,\n",
    "                                  )\n",
    "            \n",
    "                X_subset = X[best_features]\n",
    "                clf.fit(X_subset, y, params=params)\n",
    "                preds = clf.predict_proba(X_subset)[:,1]\n",
    "                ds = get_bootstrap_score(y, preds, n_bootstrap=100, \n",
    "                                     known_skew = known_skew, \n",
    "                                     forecast_time_indices = fti,\n",
    "                                     metric_mapper={'naupdc' : norm_aupdc, \n",
    "                                                    'bss' : brier_skill_score,\n",
    "                                                   })\n",
    "                scores.append(ds)\n",
    "                results[m][key] = scores\n",
    "            \n",
    "        save_fname = f'../datasets/top_predictors_{target}_{opt}_scores.pkl'\n",
    "        print(f'Saving {save_fname}...')\n",
    "        with open(save_fname, 'wb') as f:\n",
    "            pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4f3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "targets= ['tornado', 'severe_hail', 'severe_wind', 'road_surface']\n",
    "opts = ['L1_based_feature_selection_with_manual', '']\n",
    "names = [ 'unnormalized']\n",
    "\n",
    "def func(target, opt, name):\n",
    "    if target == 'road_surface':\n",
    "        opt = 'original' if opt == '' else 'reduced'\n",
    "        fname = join(DATA_BASE_PATH,f'RF_road_surface_{opt}__scores.pkl')\n",
    "    else:\n",
    "        fname = join(DATA_BASE_PATH,f'LogisticRegression_first_hour_{target}_{opt}__scores.pkl')\n",
    "    \n",
    "    try:\n",
    "        print('fname: ', fname)\n",
    "        with open(fname, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        \n",
    "        feature_importances, importances = get_feature_importances(target, opt, \n",
    "                                                                   results=results,\n",
    "                                                                   method=name\n",
    "                                                                   )\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "        results=[]\n",
    "        importances=[]\n",
    "        \n",
    "    try:\n",
    "        scores = np.array(results['scores'])\n",
    "    except:\n",
    "        scores=[]\n",
    "        \n",
    "    data = {'scores' : scores, \n",
    "            'feature_importances': feature_importances, \n",
    "            'importances' : importances\n",
    "    }\n",
    "    \n",
    "    save_fname = join(DATA_BASE_PATH,f'importances_{target}_{opt}_{name}.pkl')\n",
    "    \n",
    "    print(f'Writing {save_fname}...')\n",
    "    with open(save_fname, 'wb') as f:\n",
    "        pickle.dump(data, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb5f262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#func('road_surface', '', 'unnormalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e734d8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Importance Experiment:   0%|                                                                                  | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname: fname: fname: fname: fname: fname: fname: fname:        /work/mflora/explainability_work/datasets/LogisticRegression_first_hour_severe_hail_L1_based_feature_selection_with_manual__scores.pkl /work/mflora/explainability_work/datasets/LogisticRegression_first_hour_tornado_L1_based_feature_selection_with_manual__scores.pkl/work/mflora/explainability_work/datasets/LogisticRegression_first_hour_severe_hail___scores.pkl/work/mflora/explainability_work/datasets/LogisticRegression_first_hour_severe_wind_L1_based_feature_selection_with_manual__scores.pkl/work/mflora/explainability_work/datasets/LogisticRegression_first_hour_tornado___scores.pkl/work/mflora/explainability_work/datasets/LogisticRegression_first_hour_severe_wind___scores.pkl/work/mflora/explainability_work/datasets/RF_road_surface_original__scores.pkl/work/mflora/explainability_work/datasets/RF_road_surface_reduced__scores.pkl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(14, 1000)\n",
      "(14, 1000)\n",
      "(25, 1000)\n",
      "(25, 1000)\n",
      "(32, 1000)\n",
      "(32, 1000)\n",
      "(30, 500)\n",
      "(30, 500)\n",
      "(113, 1000)\n",
      "(113, 1000)\n",
      "(113, 1000)\n",
      "(113, 1000)\n",
      "(113, 1000)\n",
      "(113, 1000)\n",
      "(11, 500)\n",
      "(11, 500)\n",
      "Writing /work/mflora/explainability_work/datasets/importances_severe_wind_L1_based_feature_selection_with_manual_unnormalized.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Importance Experiment:  12%|█████████▎                                                                | 1/8 [00:29<03:28, 29.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /work/mflora/explainability_work/datasets/importances_tornado_L1_based_feature_selection_with_manual_unnormalized.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Importance Experiment:  25%|██████████████████▌                                                       | 2/8 [00:29<01:14, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /work/mflora/explainability_work/datasets/importances_severe_hail_L1_based_feature_selection_with_manual_unnormalized.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Importance Experiment:  38%|███████████████████████████▊                                              | 3/8 [00:30<00:35,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /work/mflora/explainability_work/datasets/importances_road_surface_original_unnormalized.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Importance Experiment:  50%|█████████████████████████████████████                                     | 4/8 [00:33<00:22,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /work/mflora/explainability_work/datasets/importances_tornado__unnormalized.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Importance Experiment:  62%|██████████████████████████████████████████████▎                           | 5/8 [00:35<00:12,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /work/mflora/explainability_work/datasets/importances_severe_hail__unnormalized.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Importance Experiment:  75%|███████████████████████████████████████████████████████▌                  | 6/8 [00:35<00:05,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /work/mflora/explainability_work/datasets/importances_severe_wind__unnormalized.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Importance Experiment:  88%|████████████████████████████████████████████████████████████████▊         | 7/8 [00:36<00:02,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /work/mflora/explainability_work/datasets/importances_road_surface_reduced_unnormalized.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Importance Experiment: 100%|██████████████████████████████████████████████████████████████████████████| 8/8 [00:54<00:00,  6.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_parallel(func, \n",
    "             args_iterator=to_iterator(targets, opts, names), \n",
    "             nprocs_to_use=len(targets)*len(opts)*len(names), \n",
    "             description='Feature Importance Experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8e6e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'tornado'\n",
    "opt = ''\n",
    "name = 'unnormalized'\n",
    "\n",
    "save_fname = join(DATA_BASE_PATH,f'importances_{target}_{opt}_{name}.pkl')\n",
    "with open(save_fname, 'rb') as f:\n",
    "    data = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93eddeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multipass__backward': array([84.27301339, 52.01720773, 71.43467199, ..., 84.25975586,\n",
       "        19.9128116 , 67.39092928]),\n",
       " 'singlepass__backward': array([0.22890405, 0.13480542, 0.17808643, ..., 0.2240216 , 0.05934309,\n",
       "        0.17213575]),\n",
       " 'multipass__forward': array([3.23199299, 2.33906528, 2.9362185 , ..., 3.28155818, 1.0577395 ,\n",
       "        2.5376998 ]),\n",
       " 'singlepass__forward': array([82.30490197, 50.42458624, 69.70266776, ..., 82.30457689,\n",
       "        19.27929793, 65.98978412]),\n",
       " 'ale_variance': array([0.23147406, 0.14182992, 0.20880368, ..., 0.23184216, 0.05566745,\n",
       "        0.16313019]),\n",
       " 'shap_sum': array([32.06791779, 21.56891473, 28.36262951, ..., 32.25782485,\n",
       "         8.42136643, 24.85530154]),\n",
       " 'sage': array([6.42116173, 4.77702071, 6.30638108, ..., 6.45591226, 1.70388128,\n",
       "        4.7656822 ]),\n",
       " 'coefs': array([33.73604638, 22.32935848, 29.36938716, ..., 34.12734362,\n",
       "         9.2388022 , 26.61489306])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['importances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e965b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
