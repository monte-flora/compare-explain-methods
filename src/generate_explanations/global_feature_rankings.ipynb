{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca37fcf2",
   "metadata": {},
   "source": [
    "## Compute the Global Feature Rankings \n",
    "\n",
    "In this notebook, we compute the global feature rankings. The methods include:\n",
    "1. ALE variance (PAPER)\n",
    "2. Backward Single-pass Permutation Importance \n",
    "3. Forward Single-pass Permutation Importance \n",
    "4. Backward Multi-pass Permutation Importance  \n",
    "5. Forward Multi-pass Permutation Importance \n",
    "6. Random Forest Gini Impurity \n",
    "7. Logistic Regression Coefficients \n",
    "8. Summed SHAP values \n",
    "9. SAGE values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7212a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "from os.path import dirname\n",
    "path = dirname(dirname(os.getcwd()))\n",
    "sys.path.insert(0, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d022dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skexplain \n",
    "from skexplain.common.importance_utils import to_skexplain_importance\n",
    "from src.io.io import load_data_and_model\n",
    "from src.common.util import subsampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08e8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants. \n",
    "N_BOOTSTRAP = 1\n",
    "N_BINS = 20 \n",
    "N_JOBS = 30 \n",
    "N_PERMUTE = 5\n",
    "SIZE = 5000\n",
    "evaluation_fn = 'norm_aupdc'\n",
    "RESULTS_PATH = os.path.join(path, 'results')\n",
    "BASE_PATH = '/work/mflora/explainability_work/'\n",
    "DATA_BASE_PATH = os.path.join(BASE_PATH, 'datasets')\n",
    "MODEL_BASE_PATH = os.path.join(BASE_PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b860bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Load the Data and Model \n",
    "dataset = 'tornado'\n",
    "option = 'reduced'\n",
    "model, X, y = load_data_and_model(dataset, option, DATA_BASE_PATH, MODEL_BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77286eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the dataset. \n",
    "X_sub, y_sub = subsampler(X,y,SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60164744",
   "metadata": {},
   "source": [
    "### 1. ALE variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09437094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method save in module skexplain.main.explain_toolkit:\n",
      "\n",
      "save(fname, data) method of skexplain.main.explain_toolkit.ExplainToolkit instance\n",
      "    Save results of a computation (permutation importance, calc_ale, calc_pd, etc)\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fname : string\n",
      "        filename to store the results in (including path)\n",
      "    data : ExplainToolkit results\n",
      "        the results of a ExplainToolkit calculation. Can be a dataframe or dataset.\n",
      "    \n",
      "    Examples\n",
      "    -------\n",
      "    >>> import skexplain\n",
      "    >>> estimators = skexplain.load_models() # pre-fit estimators within skexplain\n",
      "    >>> X, y = skexplain.load_data() # training data\n",
      "    >>> explainer = skexplain.ExplainToolkit(estimators=estimators\n",
      "    ...                             X=X,\n",
      "    ...                             y=y,\n",
      "    ...                            )\n",
      "    >>> perm_imp_results = explainer.calc_permutation_importance(\n",
      "    ...                       n_vars=10,\n",
      "    ...                       evaluation_fn = 'norm_aupdc',\n",
      "    ...                       direction = 'backward',\n",
      "    ...                       subsample=0.5,\n",
      "    ...                       n_bootstrap=20,\n",
      "    ...                       )\n",
      "    >>> fname = 'path/to/save/the/file'\n",
      "    >>> explainer.save(fname, perm_imp_results)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(explainer.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c98bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981a3a72e70e4e25b1e28954899500ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "data is not a pandas.DataFrame or xarray.Dataset. The type is <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m ale_rank \u001b[38;5;241m=\u001b[39m to_skexplain_importance(ale_var[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124male_variance_scores__\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, \n\u001b[1;32m      8\u001b[0m                                            model[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns), method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124male_variance\u001b[39m\u001b[38;5;124m'\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save the raw ALE and ALE-variance rankings results for paper 1 \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43male\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRESULTS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43male_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moption\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m explainer\u001b[38;5;241m.\u001b[39msave(ale_rank, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RESULTS_PATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124male_rank_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ai2es/lib/python3.8/site-packages/skexplain/main/explain_toolkit.py:2467\u001b[0m, in \u001b[0;36mExplainToolkit.save\u001b[0;34m(self, fname, data)\u001b[0m\n\u001b[1;32m   2465\u001b[0m     save_dataframe(fname\u001b[38;5;241m=\u001b[39mfname, dframe\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m   2466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is not a pandas.DataFrame or xarray.Dataset. The type is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2469\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: data is not a pandas.DataFrame or xarray.Dataset. The type is <class 'str'>."
     ]
    }
   ],
   "source": [
    "# Compute ALE \n",
    "explainer = skexplain.ExplainToolkit(model, X_sub, y_sub)\n",
    "ale = explainer.ale(features='all', n_bootstrap=N_BOOTSTRAP, n_bins=N_BINS)\n",
    "# Compute the variance. \n",
    "ale_var = explainer.ale_variance(ale)\n",
    "# Convert to feature rankings \n",
    "ale_rank = to_skexplain_importance(ale_var[f'ale_variance_scores__{model[0]}'].values, \n",
    "                                           model[0], list(X.columns), method='ale_variance', normalize=True)\n",
    "\n",
    "# Save the raw ALE and ALE-variance rankings results for paper 1 \n",
    "explainer.save(os.path.join(RESULTS_PATH, f'ale_{dataset}_{option}.nc'), ale)\n",
    "explainer.save(os.path.join(RESULTS_PATH, f'ale_rank_{dataset}_{option}.nc'), ale_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the permutatation importance (forward, backward, single-pass, multi-pass)\n",
    "DIRECTIONS = ['forward', 'backward']\n",
    "n_vars = len(X.columns)\n",
    "\n",
    "for direction in DIRECTIONS: \n",
    "    results = explainer.permutation_importance(\n",
    "                                           n_vars=n_vars, \n",
    "                                           evaluation_fn=evaluation_fn,\n",
    "                                           n_permute=N_PERMUTE, \n",
    "                                           n_jobs=N_JOBS,\n",
    "                                           verbose=True,\n",
    "                                           random_seed=42, \n",
    "                                           direction=direction,\n",
    "                                              )\n",
    "    # Save the results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34454150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PD \n",
    "explainer = skexplain.ExplainToolkit(model, X_sub, y_sub)\n",
    "pd = explainer.pd(features='all', n_bootstrap=N_BOOTSTRAP, n_bins=N_BINS)\n",
    "\n",
    "# Save the results\n",
    "explainer.save(pd, 'ale')\n",
    "\n",
    "# Compute \n",
    "results = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ade92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-specific \n",
    "rf.feature_importances_\n",
    "lr.coefs_\n",
    "\n",
    "to_skexplain_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP (Approx. Owen Values)\n",
    "# Check if each SHAP example can be ran in parallel. \n",
    "results = explainer.shap(shap_kwargs={'masker' : \n",
    "                                      shap.maskers.Partition(X, max_samples=100, \n",
    "                                                             clustering=\"correlation\"), \n",
    "                                     'algorithm' : 'permutation'})\n",
    "\n",
    "# Sum the SHAP values for each feature and then save results. \n",
    "\n",
    "explainer.save('shap_values.nc', data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SAGE\n",
    "def compute_sage(model, X, y, background):\n",
    "    \"\"\"Compute SAGE\"\"\"\n",
    "    # Set up an imputer to handle missing features\n",
    "    random_state = np.random.RandomState(42)\n",
    "    random_inds = np.random.choice(len(background), size=100, replace=False)\n",
    "    try:\n",
    "        X_rand = background.values[random_inds,:]\n",
    "    except:\n",
    "        X_rand = background[random_inds,:]\n",
    "    \n",
    "    # Set up the imputer. \n",
    "    imputer = sage.MarginalImputer(model.predict_proba, X_rand)\n",
    "\n",
    "    # Set up an estimator. \n",
    "    estimator = sage.PermutationEstimator(imputer, 'cross entropy')\n",
    "\n",
    "    print(np.shape(X))\n",
    "    \n",
    "    sage_values = estimator(X, y)\n",
    "    \n",
    "    return sage_values\n",
    "\n",
    "# Calculate SAGE values\n",
    "    X_sub, y_sub = subsampler(X,y)\n",
    "    sage_values = compute_sage(model, X_sub.values, y_sub, X)\n",
    "    \n",
    "    with open(out_file, 'wb') as f:\n",
    "        pickle.dump(sage_values, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
